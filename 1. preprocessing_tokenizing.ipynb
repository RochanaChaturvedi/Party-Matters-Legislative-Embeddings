{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'or', 'because', 'as', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'above', 'below', 'to', 'from', 'in', 'out', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',  'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn',  'hasn', 'haven',  'isn',  'ma', 'mightn',  'mustn',  'needn', 'shan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fulldata(fileyear):\n",
    "    with (open(\"data/full_data_\"+fileyear+\".pkl\", \"rb\")) as openfile:\n",
    "        data=pickle.load(openfile)\n",
    "    groups=vote.groupby('natural_id')\n",
    "    bills=groups.first().reset_index()\n",
    "    bills['clean_text']=bills['summary'].str.lower()\n",
    "    word_series = bills['clean_text']\n",
    "    word_series = word_series.str.replace(r'[^A-Za-z]',' ')#all except alphabet\n",
    "#     sw = stopwords.words('english')\n",
    "    word_series.fillna(value='', inplace=True)\n",
    "#     word_series = word_series.apply(lambda row:' '.join([oovDict.get(i, i) for i in row.split()]))\n",
    "    word_series=word_series.apply(lambda row:' '.join([w for w in row.split() if len(w)>1] ))\n",
    "    bills['tokenized_text']=word_series.apply(lambda row: [word for word in row.split() if word not in sw])\n",
    "    \n",
    "    bills['clip_text']=bills['tokenized_text'].map(lambda x:x[0:399])\n",
    "    bills=bills[['natural_id','clip_text']]\n",
    "       \n",
    "    vote_processed=pd.merge(vote,bills,on='natural_id',how='inner')\n",
    "    vote_processed.rename(columns={'clip_text':'clip_summary'},inplace=True) \n",
    "    with (open(\"data/full_data_\"+fileyear+\"_processed.pkl\", \"wb\")) as openfile:\n",
    "        pickle.dump(vote_processed,openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_fulldata(\"20052012\")\n",
    "preprocess_fulldata(\"20152016\")\n",
    "preprocess_fulldata(\"20132014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_text(fileyear,clip_len=2000):\n",
    "    with (open(\"bill_text_\"+fileyear+\".pkl\", \"rb\")) as openfile:\n",
    "        bill_text=pickle.load(openfile)\n",
    "    bill_text['clean_text']=bill_text['full_text'].str.lower()\n",
    "    word_series = bill_text['clean_text']\n",
    "    word_series = word_series.str.replace(r'[^A-Za-z]', ' ')\n",
    "\n",
    "    sw = stopwords.words('english')\n",
    "    bill_text['tokenized_text']=word_series.apply(lambda row: [word for word in row.split() if word not in sw])\n",
    "\n",
    "    bill_text['clip_text']=bill_text['tokenized_text'].map(lambda x:x[0:clip_len-1])\n",
    "\n",
    "    bill_text_processed=bill_text.drop(['clean_text','tokenized_text','full_text'],axis=1)\n",
    "    bill_text_processed.rename(columns={'clip_text':'text'},inplace=True)  \n",
    "    with (open(\"bill_text_\"+fileyear+\"_processed.pkl\", \"wb\")) as openfile:\n",
    "        pickle.dump(bill_text_processed,openfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_fulltext(\"20052012\")\n",
    "preprocess_fulltext(\"20152016\")\n",
    "preprocess_fulltext(\"20132014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some preliminary exploratory analysis\n",
    "# len(data['natural_id'].unique())\n",
    "# data['natural_id'][0]\n",
    "# data['summary'][0]\n",
    "# data['summary'][bills['natural_id']=='20132014r_HRES 590']\n",
    "# data.loc[0,'summary']\n",
    "# data=data[['natural_id','summary']]\n",
    "# data.head()\n",
    "# data.info()\n",
    "# for name, group in groups:\n",
    "#     print(name)\n",
    "#     print(group)\n",
    "# sum(data.vote==1.0)/len(data)#68.32 for 2005-12, 65.92 for 13-14, 61.07 for 15-16\n",
    "\n",
    "# yes_percent=groups['vote'].mean()*100\n",
    "# yes_percent.head()\n",
    "# print(yes_percent.describe())\n",
    "# sum(yes_percent<99)#all\n",
    "# counts=word_series.map(lambda x:len(x))\n",
    "# counts.idxmax()\n",
    "# bill_text_processed.loc[279,'summary']\n",
    "# counts.quantile(0.55)\n",
    "# data.loc[5,'summary']\n",
    "# data=pd.merge(vote,bill_text_processed)\n",
    "# counts=word_series.map(lambda x:len(x))\n",
    "# counts.max()\n",
    "# vote_processed.info()\n",
    "# data.info()\n",
    "# bills.info()\n",
    "# print(bill_text.head())\n",
    "# #print(bill_text.info())\n",
    "# count=bill_text['full_text'].str.count(\":\")\n",
    "# #bill_text['clean_text'].head()\n",
    "# #bill_text.loc[359] empty no text\n",
    "# counts=bill_text['tokenized_text'].map(lambda x:len(x))\n",
    "# #sum(counts>2000) #op 721\n",
    "# #counts.len()\n",
    "# #counts.quantile(0.999)\n",
    "# #count=2000\n",
    "# #\n",
    "# print(counts.min())\n",
    "# print(count.idxmin())\n",
    "# print(bill_text.iloc[count.idxmin()])\n",
    "# #print(counts[counts.idxmin])\n",
    "# #print(counts.idxmin())\n",
    "# #counts.describe()\n",
    "# #bill_text.iloc[1568]['tokenized_text']\n",
    "# #print(counts.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
